# Q&A - 4/11

Because of singularity [..] AI gone wild [..] the world will witness a
calamity. Robots will kill us.

Unlikely

AI software is just like any other software - it can have
bugs. Especially in the scale some people talk about this stuff, it
will lots, which will make it vulnerable to attacks. "But it can
evolve dude, like, become more awesome each time". Yeah, but then it
will be like AI with bugs building AI with probably more bugs. The
"safest" way for these bots would be to evolve through natural
selection (nature is the most stringent QA tester), but, we've been
evolving like that forÂ  years, and we still have bugs, and not enough
features. I mean, bats can fly blind with sonar, some animals see
wider spectrum of color.. We dont have any of it. I'd like to detect
movement behind me with sonar coming out of my ass; but I can't. It
would be an evolutionary advantage if u think about it. Say
hunter-gatherer is taking a dump out in the woods, then ... he
"senses" something.. "I detect incoming tiger in my 6 o'clock
sir".. "Roger that. Mission abort. Take your dump elsewhere".

This is not to say custom-made, special purpose AI isnt cool, and wont
be extremely useful to the point that it will displace (boring,
non-creative) jobs. The words "special purpose" are key
though. Self-driving cars are here. Playing chess, winning Jeopardy,
great. But can the chess playing AI drive the car, or play jeopardy?
Rule #1 of machine learning: No Silver Bullet. No single method solves
all problems equally well.

And, between now and this "singularity" point, we will have plenty of
time to interject, design whatever this AI can do.. Isaac Asimov did
nothing else but try to outline the parameters for this future, with
his robot laws etc.
















