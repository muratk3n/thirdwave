# Q&A - 14/12

Ban-ki Moon

Excellencies:

When historians look back on this day, they will say that global cooperation to secure a future safe from climate change took a dramatic new turn here in Paris.

Today, we can look into the eyes of our children and grandchildren, and we can finally, after so many years of discussion and delay, tell them that we have joined hands to bequeath a more habitable world to them and to future generations.

We have an agreement. It is a good agreement. You should all be proud.

Now we must stay united -- and bring the same spirit to the crucial test of implementation.

That work starts tomorrow.

For today, congratulations again on a job well done.

Let us work together, with renewed commitment, to make this a better world for all.

Thank you.

Merci.

Congrats

Nick Brown

Daniel Kahneman's warning of a looming train wreck in social psychology took another step closer towards realisation today with the publication of this opinion piece [..] entitled "Your iPhone Is Ruining Your Posture — and Your Mood" [in the New York Times].

[In it], Professor Amy Cuddy of Harvard Business School reports on "preliminary research" [..] that she performed with her colleague, Maarten Bos.  Basically, they gave some students some Apple gadgets to play with, ranging in size from an iPhone up to a full-size desktop computer.  The experiment gave the participants some filler tasks, and then left, telling them that he would be back in five minutes to debrief them, but that they could also come and get him at his desk.  He then didn't come back after five minutes as announced, but instead waited ten minutes.  The main outcome variable was whether the participants came to get him, and how long people they waited before doing so, as a function of the size of the device that they had.  It turned out that, the smaller the device, the longer they waited [..]

It's hard to know where to begin with this.  There are other plausible explanations, starting with the fact that a lot of people don't have an iPhone and might well enjoy playing with one compared to their Android phone, whereas a desktop computer is still just a desktop computer, even if it is a Mac.  And the effect size was pretty large: the partial eta-squared of the headline result is .177, which should be compared to Cohen's (1988) description of a partial eta-squared of .14 as a "large" effect.  Oh, and there were 75 participants in four conditions, making a princely 19 per cell.  In other words, all the usual suspect things about priming studies.

But what I find really annoying here is that we've gone straight from "preliminary research" to the New York Times without any of those awkward little academic niceties such as "peer review".  The article, in "working paper" form (1,000 words) is here; check out the date (May 2013) and ask yourself why this is suddenly front-page news when, after 30 months, the authors don't seem to have had time to write a proper article and send it to a journal, although one of them did have time to write 845 words for an editorial in the New York Times.  But perhaps those 845 words didn't all have to be written from scratch, because — oh my, surprise surprise — Professor Cuddy is "the author of the forthcoming book 'Presence: Bringing Your Boldest Self to Your Biggest Challenges.'"  Anyone care to take a guess as to whether this research will appear in that book, and whether its status as an unreviewed working paper will be prominently flagged up?

If this is the future — writing up your study pro forma and getting it into what is arguably the world's leading newspaper, complete with cute message that will appeal to anyone who thinks that everybody else uses their smartphone too much — then maybe we should just bring on the train wreck now.

Darn

Not that the peer review process is the  bestest filter around; but there is something wrong with this half-baked research getting out there. 

NYT

The [self-driving] cars now being tested by Google, BMW, Ford and others all see by way of a particular kind of scanning system called lidar (a portmanteau of ‘‘light’’ and ‘‘radar’’). A lidar scanner sends out tiny bursts of illumination invisible to the human eye, almost a million every second, that bounce off every building, object and person in the area. This undetectable machine-­flicker is ‘‘capturing’’ extremely detailed, millimeter-­scale measurements of the surrounding environment, far more accurate than anything achievable by the human eye.

Yes

The excerpt above accentuates the point that  self-driving cars are not yet driving like humans; their AI solves a simpler problem, by relying on more accurate sensor data that contains, within it, accurate distance information.  Obviously self-driving car tech is still an  incredible achivement, but there is a long way to go to replicate humans. 

Humans can drive simply by using  their eyes as sensors, even one eye because we are able to perform tasks that are known as "structure from motion" in the vision literature (or visual SLAM, or 3D reconstruction, or monocular vision, etc). I look at a building, extract "interesting points" from this image, take a step, look again, extract the same features, they would have shifted in my 2D vision, my eye. Extrapolating from my step, the 2D dislocation, I can roughly compute a 3D model of the environment. More images would increase the accuracy. 

This is not an easy task.  In fact nearly half of a human's  cortex activity is dedicated to vision [1]. SfM  is an active area of research (mathematicians - jump in!), and an area of interest for me personally (I like doing more with less, human-like 3D reconstruction from one bloddy camera, it's beyond cool IMO).

Note: there have been self-driving cars that are based on pure vision, and vision is being used in current state-of-art self-driving cars to complement Lidar. But there is no human-like pure vision driven system out there ready for commercial use.  

News

Would-be presidential nominee and full-on American bigot “The Donald” has gotten so out of hand with his derision toward women, African-Americans, Mexicans, the press, the disabled, and now calling for a ban on all Muslims, that Anonymous is starting to take some of its focus off Daesh (formerly known as the Islamic State) and focusing it right here on our very own terrorist-in-the-making, Donald Trump.


David Keirsey

Because of their utilitarian character, [MBTI] Artisans will strike off down roads that others might consider impossible, tackling problems, making deals, clearing hurdles, knocking down barriers-doing whatever it takes (authorized or unauthorized) to bull their way through to a successful outcome. One prominent State Department negotiator has exhibited all of these SP traits in his roller-coaster career:

He has yelled at Foreign Ministers and cursed at a President. He has negotiated agreements of immense consequence on the fly, making them up as he goes along ... betting on himself and the deal in hand at two o'clock in the morning. He has politely negotiated with killers and, by his own account, at least one psychopath .... He has shamelessly and effectively exploited the media .. .in order to promote American policy aims and to intimidate those who stood in his way.

No high-flown speculation for the Artisan, no deep meaning or introspection. Leave to others the protocol, the scientific inquiry, the inward search. SPs focus on what actually happens in the real world, on what works, on what pays off, and not on whose toes get stepped on, what principles are involved, or why things happen [2].

Yes

Trump, an ESTP, shows all of the traits above. This might all be okay in their little  world, or in more tactical scenarios, but is this sort of behavior fitting for a President? Bush was another ESTP let's not forget (and he shares another trait with "The Trump" that I might touch on later). Dubya was less of a loud mouth, but he did f**k it all up in a rather STP kinda way.

---

[1] Felleman, D. J. and van Essen, D. C. (1991). Distributed hierarchical processing in the primate cerebral cortex. Cerebral Cortex, I : 1-47.

[2] Keirsey, Please Understand Me II







at

December 14, 2015















