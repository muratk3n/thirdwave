# More Data = Better Models?

Newer approaches, methods excite people -- in such a degree that some start thinking that any other method that came before was useless. I hear such talk especially related to pattern recognition and I'd like to advise extreme caution in such matters. Some talk about "lots of data make math modeling unnecessary", this is entirely idoatic.

In pattern recognition applications, more data can mean better pattern recognition. I watched Peter Norvig of Google talk about how lots of data made a huge difference for speech recognition. It was a great talk, there is a certain craft in bulding a model that will *allow* more data to result in better recognition. This is awesome research.

But this is recognition, so by definition, more data can make a difference. Let's take humans as example, if we have more things in our heads, we can recognize more things -- naturally. A child starts out with little vocabulary, gradually builds it up and finally becomes a walking recognizing machine who can understand anything.

But, not all mathematical models are limited to this. For example let's use Hubbard's model for Peak Oil.. When Hubbard built that model and calculated that US would experience Peak Oil during 70s, there was no data for any year after that. So more data could not have made any difference, because Hubbard could not have any data for the period he was investigating. He made some very smart assumptions, included those assumptions in form of an equations (a great skill on its own), and voila! He found the exact time period.

